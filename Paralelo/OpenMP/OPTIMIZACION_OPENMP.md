# üöÄ Informe de Optimizaci√≥n OpenMP para MLP

**Fecha:** 8 de diciembre de 2025  
**Proyecto:** HPC-MLP-Engine  
**Autor:** Optimizaci√≥n realizada con asistencia de GitHub Copilot

---

## üìä Resultados Obtenidos

| Versi√≥n | Tiempo Total | Speedup |
|---------|-------------|---------|
| OpenMP sin optimizar | 313.5 segundos | 0.22x (m√°s lento) |
| C Secuencial | 70.0 segundos | 1.0x (baseline) |
| **OpenMP Optimizado** | **25.3 segundos** | **2.8x** |

**Mejora total:** De 313s a 25s = **12.4x m√°s r√°pido**

---

## üî¥ Problema Inicial: ¬øPor qu√© OpenMP era M√ÅS LENTO?

### El Anti-patr√≥n: Sobre-paralelizaci√≥n

```c
// ‚ùå MALO: Paralelizar TODO con OpenMP
#pragma omp parallel for collapse(2)
for (int i = 0; i < rows; i++) {
    for (int j = 0; j < cols; j++) {
        // operaci√≥n peque√±a
    }
}
```

**Cada `#pragma omp parallel for` tiene un costo:**
1. **Creaci√≥n/destrucci√≥n de threads** (~1-10 Œºs)
2. **Sincronizaci√≥n de barreras** (~0.5-5 Œºs)
3. **False sharing en cach√©** (muy costoso)
4. **Overhead del runtime OpenMP**

Con `batch_size=32` y miles de operaciones peque√±as por epoch, el overhead acumulado **superaba el beneficio** del paralelismo.

---

## ‚úÖ Soluci√≥n: Paralelizaci√≥n Inteligente

### Principio 1: Solo paralelizar cuando hay suficiente trabajo

```c
// ‚úÖ BUENO: Threshold basado en cantidad de trabajo
const int work = rows_A * cols_A * cols_B;

if (rows_A >= 256 && work >= 1000000) {
    #pragma omp parallel for schedule(static)
    for (int i = 0; i < rows_A; i++) {
        // ... trabajo pesado
    }
} else {
    // Versi√≥n secuencial para trabajo peque√±o
}
```

**Regla pr√°ctica:** Solo paralelizar si:
- `batch_size >= 256` (suficientes filas independientes)
- `trabajo_total >= 1,000,000` operaciones

### Principio 2: Usar SIMD en lugar de threads para loops peque√±os

```c
// ‚úÖ BUENO: SIMD para vectorizaci√≥n sin overhead de threads
#pragma omp simd
for (int j = 0; j < cols_B; j++) {
    c_row[j] += a_ik * b_row[j];
}
```

**`#pragma omp simd`** le dice al compilador:
- "Este loop es seguro de vectorizar"
- Usa instrucciones AVX/SSE (procesa 8 floats a la vez)
- **Cero overhead de threads**

### Principio 3: Reducci√≥n paralela correcta

```c
// ‚úÖ BUENO: reduction para sumas paralelas
float sum = 0.0f;
#pragma omp simd reduction(+:sum)
for (int k = 0; k < next_neurons; k++) {
    sum += D_row[k] * W_row[k];
}
```

---

## üîß Cambios Espec√≠ficos Realizados

### 1. `linalg.c` - Multiplicaci√≥n de Matrices

#### mat_mul (Forward Pass)

```c
// ANTES: Siempre paralelo con threshold muy bajo
if (rows_A >= 128) {
    #pragma omp parallel for  // ‚ùå Overhead alto
}

// DESPU√âS: Threshold inteligente + SIMD
const int work = rows_A * cols_A * cols_B;
if (rows_A >= 256 && work >= 1000000) {
    #pragma omp parallel for schedule(static)
    for (int i = 0; i < rows_A; i++) {
        // Cada thread trabaja en su fila (sin conflictos)
        float* restrict c_row = &c_data[i * cols_B];
        for (int j = 0; j < cols_B; j++) c_row[j] = 0.0f;
        
        for (int k = 0; k < cols_A; k++) {
            #pragma omp simd  // ‚úÖ Vectorizaci√≥n del loop interno
            for (int j = 0; j < cols_B; j++) {
                c_row[j] += a_ik * b_row[j];
            }
        }
    }
}
```

#### mat_mul_AtB (Backpropagation - Gradientes)

Esta es la operaci√≥n **M√ÅS PESADA** del entrenamiento:
- Dimensiones: `256 √ó 784 √ó 500 = 100,352,000` operaciones
- Aqu√≠ s√≠ vale la pena paralelizar

```c
// Paralelizar cuando hay trabajo significativo
if (cols_A >= 500 && cols_B >= 10) {
    #pragma omp parallel for schedule(static)
    for (int i = 0; i < cols_A; i++) {  // 784 iteraciones paralelas
        // Cada thread calcula una fila completa de gradientes
        float* restrict c_row = &c_data[i * cols_B];
        for (int j = 0; j < cols_B; j++) c_row[j] = 0.0f;
        
        for (int k = 0; k < rows_A; k++) {
            #pragma omp simd
            for (int j = 0; j < cols_B; j++) {
                c_row[j] += a_ki * b_row[j];
            }
        }
    }
}
```

### 2. `network.c` - Propagaci√≥n de Error

#### Backpropagation en Capas Ocultas

```c
// ANTES: Loop triple secuencial
for (int r = 0; r < batch; r++) {
    for (int c = 0; c < curr_neurons; c++) {
        float sum = 0.0f;
        for (int k = 0; k < next_neurons; k++) {
            sum += D[r * next_neurons + k] * W[c * next_neurons + k];  // ‚ùå Acceso no √≥ptimo
        }
    }
}

// DESPU√âS: Paralelo por batch + acceso optimizado a memoria
#pragma omp parallel for schedule(static) if(batch >= 128 && curr_neurons >= 100)
for (int r = 0; r < batch; r++) {
    // Punteros locales para mejor localidad de cach√©
    const float* restrict D_row = &D[r * next_neurons];
    const float* restrict A_row = &A_data[r * curr_neurons];
    float* restrict err_row = &err[r * curr_neurons];
    
    for (int c = 0; c < curr_neurons; c++) {
        float sum = 0.0f;
        const float* restrict W_row = &W[c * next_neurons];
        
        #pragma omp simd reduction(+:sum)  // ‚úÖ Reducci√≥n vectorizada
        for (int k = 0; k < next_neurons; k++) {
            sum += D_row[k] * W_row[k];
        }
        
        const float a_val = A_row[c];
        err_row[c] = sum * a_val * (1.0f - a_val);  // Sigmoid derivative inline
    }
}
```

### 3. Operaciones Elementales - Solo SIMD

```c
// Operaciones peque√±as: NO usar parallel for, SOLO simd
void mat_add(Matrix* restrict A, const Matrix* restrict B) {
    const int size = A->rows * A->cols;
    #pragma omp simd  // ‚úÖ Vectoriza sin overhead de threads
    for (int i = 0; i < size; i++) {
        a[i] += b[i];
    }
}

void apply_sigmoid(Matrix* m) {
    #pragma omp simd
    for (int i = 0; i < size; i++) {
        data[i] = 1.0f / (1.0f + expf(-data[i]));
    }
}
```

---

## üìà An√°lisis de Trabajo por Operaci√≥n

| Operaci√≥n | Dimensiones | Trabajo | ¬øParalelizar? |
|-----------|-------------|---------|---------------|
| Forward: X√óW‚ÇÅ | 256√ó784√ó500 | 100M | ‚úÖ S√≠ |
| Forward: H√óW‚ÇÇ | 256√ó500√ó10 | 1.3M | ‚úÖ S√≠ |
| Backward: Error prop | 256√ó500√ó10 | 1.3M | ‚úÖ S√≠ |
| Backward: dW‚ÇÅ | 784√ó256√ó500 | 100M | ‚úÖ S√≠ |
| Backward: dW‚ÇÇ | 500√ó256√ó10 | 1.3M | ‚ö†Ô∏è Borderline |
| Add bias | 256√ó500 | 128K | ‚ùå Solo SIMD |
| Sigmoid | 256√ó500 | 128K | ‚ùå Solo SIMD |
| Update weights | 784√ó500 | 392K | ‚ùå Solo SIMD |

---

## üéØ Configuraci√≥n √ìptima

### Flags de Compilaci√≥n

```bash
gcc main.c ... -o mlp_openmp \
    -O3                 # Optimizaci√≥n agresiva
    -march=native       # Instrucciones espec√≠ficas de tu CPU
    -ffast-math         # Optimizaciones matem√°ticas (menos precisi√≥n)
    -funroll-loops      # Desenrollar loops peque√±os
    -fopenmp            # Habilitar OpenMP
    -Wall               # Warnings
```

### Variables de Entorno

```bash
export OMP_NUM_THREADS=4        # 4 threads suele ser √≥ptimo
export OMP_SCHEDULE=static      # Distribuci√≥n est√°tica (predecible)
export OMP_PROC_BIND=close      # Threads cerca en la topolog√≠a de CPU
```

### Par√°metros del Modelo

```c
int batch_size = 256;  // CLAVE: Mayor batch = m√°s trabajo paralelo
```

---

## üìö Lecciones Aprendidas

### 1. El overhead de OpenMP es REAL
- Cada `#pragma omp parallel` tiene costo de ~1-10 Œºs
- Con miles de llamadas por epoch, se acumula

### 2. SIMD es "paralelismo gratis"
- `#pragma omp simd` no crea threads
- El compilador usa instrucciones vectoriales (AVX)
- Procesa 8 floats simult√°neamente

### 3. Batch size importa MUCHO
- `batch=32`: No hay suficiente trabajo ‚Üí overhead domina
- `batch=256`: Suficiente trabajo ‚Üí speedup real

### 4. Acceso a memoria es cr√≠tico
- Usar punteros `restrict` para indicar no-aliasing
- Acceso secuencial por filas (row-major order)
- Evitar false sharing entre threads

### 5. Medir antes de optimizar
- Siempre comparar con versi√≥n secuencial
- Si paralelo < secuencial ‚Üí hay un problema

---

## üîÆ Posibles Mejoras Futuras

1. **OpenMP Tasks** para pipeline de batches
2. **SIMD intr√≠nsicos** (AVX2/AVX-512) para control total
3. **Blocking/Tiling** para mejor uso de cach√© L1/L2
4. **Numa-aware allocation** para sistemas multi-socket
5. **OpenBLAS/MKL** para multiplicaci√≥n de matrices optimizada

---

## üìé Referencias

- [OpenMP 5.0 Specification](https://www.openmp.org/spec-html/5.0/openmp.html)
- [Intel Optimization Manual](https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html)
- [Agner Fog's Optimization Manuals](https://www.agner.org/optimize/)

---

*Este documento fue generado como parte del proceso de optimizaci√≥n del proyecto HPC-MLP-Engine.*
